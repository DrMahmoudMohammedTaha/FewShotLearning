{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VokdXWMMrTfG"
      },
      "source": [
        "# Relation Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z58GmYCFrTfK"
      },
      "source": [
        "Relation Networks (RNs) are designed for few-shot learning by explicitly learning relationships between query and support examples. They use a neural network to dynamically model similarity, enabling flexible and effective classification with minimal labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcfJKhKJrTfM"
      },
      "source": [
        "##### Step 1: Import all necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UxL-GYeorTfO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0GJji7bxk07",
        "outputId": "5c02769b-9d86-4d8c-a782-f4073f6af1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS28NIQqrTfQ"
      },
      "source": [
        "#### Step 2: We will load RHA dataset, tranformed in .npy format using helper script.  \n",
        "In helper script, we are just loading data in size format: [total_number,character,64,64].                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kHpggb_H2K1_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Function to create dataset from folders\n",
        "# def create_npy_from_folders(data_dir, img_size=(224, 224)):\n",
        "#     all_classes = sorted(os.listdir(data_dir))\n",
        "#     all_images = []\n",
        "#     min_samples = float('inf')\n",
        "\n",
        "#     for class_folder in all_classes:\n",
        "#         class_path = os.path.join(data_dir, class_folder)\n",
        "#         if not os.path.isdir(class_path):\n",
        "#             continue\n",
        "#         class_images = []\n",
        "#         for img_file in sorted(os.listdir(class_path)):\n",
        "#             img_path = os.path.join(class_path, img_file)\n",
        "#             if img_file.endswith(('.png', '.jpg', '.jpeg')):\n",
        "#                 try:\n",
        "#                     img = Image.open(img_path).convert('L')\n",
        "#                     img = img.resize(img_size)\n",
        "#                     class_images.append(np.array(img))\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"Error loading image {img_path}: {e}\")\n",
        "#                     continue\n",
        "#         min_samples = min(min_samples, len(class_images))\n",
        "#         all_images.append(class_images)\n",
        "\n",
        "#     truncated_images = [class_images[:min_samples] for class_images in all_images]\n",
        "#     dataset = np.array(truncated_images)\n",
        "#     np.save('data.npy', dataset)\n",
        "#     return dataset\n",
        "\n",
        "# # Load the dataset\n",
        "# data_dir = '/content/drive/MyDrive/_projects/GEI_Project/Dataset_fewshot'\n",
        "# dataset = create_npy_from_folders(data_dir, img_size=(224, 224))\n",
        "\n",
        "# print(f\"Dataset shape: {dataset.shape}\")\n",
        "\n",
        "# # Split each class separately\n",
        "# x_train, x_val, x_test = [], [], []\n",
        "# y_train, y_val, y_test = [], [], []\n",
        "\n",
        "# for class_idx in range(dataset.shape[0]):\n",
        "#     class_samples = dataset[class_idx]\n",
        "#     train, temp = train_test_split(class_samples, test_size=0.3, random_state=42)\n",
        "#     val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "#     x_train.append(train)\n",
        "#     x_val.append(val)\n",
        "#     x_test.append(test)\n",
        "\n",
        "#     y_train.extend([class_idx] * len(train))\n",
        "#     y_val.extend([class_idx] * len(val))\n",
        "#     y_test.extend([class_idx] * len(test))\n",
        "\n",
        "# # Convert lists to numpy arrays\n",
        "# x_train = np.array(x_train)\n",
        "# x_val = np.array(x_val)\n",
        "# x_test = np.array(x_test)\n",
        "# y_train = np.array(y_train)\n",
        "# y_val = np.array(y_val)\n",
        "# y_test = np.array(y_test)\n",
        "\n",
        "# print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
        "# print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
        "# print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q_XW-QirTfV"
      },
      "source": [
        "Let's Visualize example of each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yl9_BQR5rTfW"
      },
      "outputs": [],
      "source": [
        "# for class_idx in range(x_train.shape[0]):  # Loop over classes (3 classes in your case)\n",
        "#     print(\"class \" + str(class_idx))\n",
        "#     # Extract the first image from the current class\n",
        "#     first_image = x_train[class_idx, 0, :, :]  # (height, width)\n",
        "\n",
        "#     # Plot the first image of each class\n",
        "#     plt.figure()\n",
        "#     plt.imshow(first_image, cmap='gray')  # Display as grayscale\n",
        "#     plt.title(f\"Class {class_idx}, Image 0\")  # Add a title with class index and image number\n",
        "#     plt.axis('off')  # Hide axis for better visualization\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a70sEUxrTfX"
      },
      "source": [
        "###### Step 3: Training Data Processing\n",
        "To Load dataset, and prepare it for Relation Networks Architecture, we need to create:\n",
        "1. Label Set: Variable choose_label\n",
        "2. Support Set: support_set_x, support_set_y\n",
        "3. Batch from Suppport Set Examples\n",
        "\n",
        "Let's first create a batch which can give a support set, and target set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnpLTuugrTfZ"
      },
      "source": [
        "##### Step 3: Create a Relation Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "BG6VmeU7rTfZ"
      },
      "outputs": [],
      "source": [
        "# Custom dataset for loading images from folders\n",
        "class FewShotDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(root_dir)\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        for label, class_dir in enumerate(self.classes):\n",
        "            class_path = os.path.join(root_dir, class_dir)\n",
        "            for img_name in os.listdir(class_path):\n",
        "                self.image_paths.append(os.path.join(class_path, img_name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor (e.g., CNN)\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        return out.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "# Relation module\n",
        "class RelationModule(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RelationModule, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Few-shot relation network\n",
        "class RelationNetwork(nn.Module):\n",
        "    def __init__(self, feature_dim, relation_dim):\n",
        "        super(RelationNetwork, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.relation_module = RelationModule(feature_dim + relation_dim)\n",
        "\n",
        "    def forward(self, support, query):\n",
        "        support_features = self.feature_extractor(support)  # (N_way * K_shot, feature_dim)\n",
        "        query_features = self.feature_extractor(query)      # (N_query, feature_dim)\n",
        "\n",
        "        N_query = query_features.size(0)\n",
        "        N_support = support_features.size(0)\n",
        "\n",
        "        query_features = query_features.unsqueeze(1).expand(-1, N_support, -1)\n",
        "        support_features = support_features.unsqueeze(0).expand(N_query, -1, -1)\n",
        "\n",
        "        combined = torch.cat((query_features, support_features), dim=-1)\n",
        "        relations = self.relation_module(combined.view(-1, combined.size(-1)))\n",
        "        return relations.view(N_query, N_support)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmReHKs3rTfa"
      },
      "source": [
        "##### Step 3: Training the Relation Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "width = 224\n",
        "height = 224\n",
        "data_path = '/content/drive/MyDrive/_projects/GEI_Project/Dataset_fewshot'\n",
        "total_epoches = 50\n",
        "\n",
        "# Metrics storage\n",
        "train_loss = []\n",
        "train_accuracy = []"
      ],
      "metadata": {
        "id": "RTQTM4RE9fIm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "PDAfzBoxrTfa"
      },
      "outputs": [],
      "source": [
        "# Training example\n",
        "def train_relation_network():\n",
        "    # Define image transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((width, height)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = FewShotDataset(root_dir=data_path, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    # Initialize model\n",
        "    feature_dim = 64 * 56 * 56  # Derived from conv + pooling\n",
        "    model = RelationNetwork(feature_dim=feature_dim, relation_dim=feature_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(total_epoches):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            support, query = batch[0][:8], batch[0][8:]\n",
        "            support_labels, query_labels = batch[1][:8], batch[1][8:]\n",
        "\n",
        "            relations = model(support, query)\n",
        "            target = (query_labels.unsqueeze(1) == support_labels.unsqueeze(0)).float()\n",
        "\n",
        "            loss = criterion(relations, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted = (relations > 0.5).float()\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.numel()\n",
        "\n",
        "        epoch_accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "        if epoch_accuracy > 0.7:\n",
        "          break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "a0K-HQhV8a5U",
        "outputId": "ae2b2743-98e0-460e-92ed-8d2d9ff8f3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 616.3077, Accuracy: 0.6712\n",
            "Epoch 2, Loss: 610.9375, Accuracy: 0.6807\n",
            "Epoch 3, Loss: 637.5000, Accuracy: 0.6635\n",
            "Epoch 4, Loss: 639.0625, Accuracy: 0.6678\n",
            "Epoch 5, Loss: 632.8125, Accuracy: 0.6661\n",
            "Epoch 6, Loss: 637.5000, Accuracy: 0.6661\n",
            "Epoch 7, Loss: 635.9375, Accuracy: 0.6695\n",
            "Epoch 8, Loss: 578.1250, Accuracy: 0.7012\n",
            "Epoch 9, Loss: 610.9375, Accuracy: 0.6755\n",
            "Epoch 10, Loss: 620.3125, Accuracy: 0.6755\n",
            "Epoch 11, Loss: 614.0625, Accuracy: 0.6815\n",
            "Epoch 12, Loss: 632.8125, Accuracy: 0.6635\n",
            "Epoch 13, Loss: 609.3750, Accuracy: 0.6866\n",
            "Epoch 14, Loss: 646.8750, Accuracy: 0.6661\n",
            "Epoch 15, Loss: 576.5625, Accuracy: 0.6918\n",
            "Epoch 16, Loss: 657.8125, Accuracy: 0.6473\n",
            "Epoch 17, Loss: 670.3125, Accuracy: 0.6635\n",
            "Epoch 18, Loss: 678.1250, Accuracy: 0.6336\n",
            "Epoch 19, Loss: 585.9375, Accuracy: 0.6892\n",
            "Epoch 20, Loss: 626.5625, Accuracy: 0.6721\n",
            "Epoch 21, Loss: 623.4375, Accuracy: 0.6712\n",
            "Epoch 22, Loss: 618.7500, Accuracy: 0.6738\n",
            "Epoch 23, Loss: 678.1250, Accuracy: 0.6438\n",
            "Epoch 24, Loss: 604.6875, Accuracy: 0.6841\n",
            "Epoch 25, Loss: 651.5625, Accuracy: 0.6558\n",
            "Epoch 26, Loss: 632.8125, Accuracy: 0.6635\n",
            "Epoch 27, Loss: 650.0000, Accuracy: 0.6644\n",
            "Epoch 28, Loss: 640.6250, Accuracy: 0.6592\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3ca2ef19be48>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_relation_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-9d84d3670f4c>\u001b[0m in \u001b[0;36mtrain_relation_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Run training\n",
        "train_relation_network()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFrF6k4mrTfb"
      },
      "source": [
        "#### Step 4: Testing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OfLoiaerTfe"
      },
      "source": [
        "##### Let's Run Experiments !!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqluVWLwrTfe"
      },
      "source": [
        "Now Let's obtain our test accuracy by running the following code block:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Tb62G7rTfe"
      },
      "source": [
        "#### Step 5: Let's visualize our results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mJP4urVerTff"
      },
      "outputs": [],
      "source": [
        "# Function to plot loss and accuracy\n",
        "def plot_metrics(train, val=None, name1=\"train\", name2=\"val\", title=\"\"):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.plot(train, label=name1)\n",
        "    if val is not None:\n",
        "        plt.plot(val, label=name2)\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(title.split()[0])  # Use \"Loss\" or \"Accuracy\" as y-label\n",
        "    plt.show()\n",
        "\n",
        "    # Plot loss and accuracy\n",
        "    plot_metrics(train_loss, title=\"Loss Graph\")\n",
        "    plot_metrics(train_accuracy, title=\"Accuracy Graph\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}